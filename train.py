# train.py
import argparse

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model

# ä½¿ç”¨å¼€æ”¾å¯ç›´æ¥è®¿é—®çš„æ¨¡å‹
MODEL_LIST = {
    "bert": "bert-base-uncased",
    "qwen": "Qwen/Qwen2-1.5B",
    # ç”¨ TinyLlama ä»£æ›¿ Meta çš„ gated Llama-3
    "llama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
}


def encode(sample, tokenizer):
    """
    æŠŠä¸€è¡Œæ ·æœ¬è½¬æˆæ¨¡å‹è¾“å…¥ï¼š
    è¾“å…¥ = facts + rules + questions
    label = å¦‚æœ answers é‡Œé¢åŒ…å« 'T' å°±è®¾ä¸º 1ï¼Œå¦åˆ™ 0
    ä½ å¯ä»¥æŒ‰éœ€è¦ä¿®æ”¹è¿™ä¸ªé€»è¾‘ã€‚
    """
    text = sample["facts"] + " " + sample["rules"] + " " + sample["questions"]
    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=512,
    )
    sample["input_ids"] = enc["input_ids"]
    sample["attention_mask"] = enc["attention_mask"]

    # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼šåªè¦æœ‰ T å°±ç®— True
    sample["labels"] = 1 if "T" in sample["answers"] else 0
    return sample


def build_lora_config(model_name: str) -> LoraConfig:
    """
    æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ LoRA target_modules
    - å¯¹äº decoder-onlyï¼ˆQwen / LLaMA/TinyLlamaï¼‰ä½¿ç”¨ q/k/v/o_proj
    - å¯¹äº BERT è¿™ç±» encoder-onlyï¼Œä½¿ç”¨ query / value
    """
    lower_name = model_name.lower()
    if "llama" in lower_name or "qwen" in lower_name or "tinyllama" in lower_name:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    else:
        target_modules = ["query", "value"]

    lora = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="SEQ_CLS",
    )
    return lora


def train(model_key: str = "bert"):
    model_name = MODEL_LIST[model_key]
    print(f"â–¶ Using base model: {model_name}")

    # ------------------ tokenizer ------------------
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Qwen / LLaMA / TinyLlama æœ‰å¯èƒ½æ²¡æœ‰ pad_tokenï¼Œè¿™é‡Œæ˜¾å¼è®¾ç½®ä¸º eos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        print(f"âš ï¸ pad_token was missing, set pad_token = eos_token = {tokenizer.pad_token}")

    # ------------------ dataset ------------------
    dataset = load_dataset("csv", data_files="train.csv")["train"]

    dataset = dataset.map(
        lambda x: encode(x, tokenizer),
        remove_columns=dataset.column_names,
    )

    # ------------------ model ------------------
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
    )

    # ç¡®ä¿æ¨¡å‹ä¹ŸçŸ¥é“ pad_token_id
    model.config.pad_token_id = tokenizer.pad_token_id

    # ------------------ LoRA ------------------
    lora_config = build_lora_config(model_name)
    model = get_peft_model(model, lora_config)
    print("â–¶ LoRA enabled with config:", lora_config)

    # ------------------ training args ------------------
    output_dir = f"./trained_models/{model_key}"
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        num_train_epochs=3,
        learning_rate=2e-5,
        save_strategy="epoch",
        logging_steps=20,
        remove_unused_columns=False,  # æˆ‘ä»¬å·²ç»æ‰‹åŠ¨æ§åˆ¶è¾“å…¥åˆ—
    )

    # ------------------ trainer ------------------
    trainer = Trainer(
        model=model,
        args=args,
        tokenizer=tokenizer,
        train_dataset=dataset,
    )

    # ------------------ train ------------------
    print("ğŸš€ Start training...")
    trainer.train()

    # ------------------ save ------------------
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… Model saved to: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        type=str,
        default="bert",
        choices=["bert", "qwen", "llama"],
        help="Which base model to fine-tune",
    )
    args = parser.parse_args()

    train(args.model)
