# train.py
import argparse

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model

# ä½¿ç”¨å¼€æ”¾å¯ç›´æ¥è®¿é—®çš„æ¨¡å‹
MODEL_LIST = {
    "bert": "bert-base-uncased",
    "qwen": "Qwen/Qwen2-1.5B",
    # ç”¨ TinyLlama ä»£æ›¿ Meta çš„ gated Llama-3
    "llama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
}


def encode(sample, tokenizer):
    """
    FIXED: å°†æ¯è¡Œçš„å¤šä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹æ‹†åˆ†æˆç‹¬ç«‹çš„è®­ç»ƒæ ·æœ¬
    æ¯ä¸ªé—®é¢˜å¯¹åº”ä¸€ä¸ªæ ‡ç­¾ï¼ˆT=1, F=0ï¼‰
    """
    facts = sample["facts"]
    rules = sample["rules"]
    questions = sample["questions"].split(" | ")
    answers = sample["answers"].split(" | ")

    # åˆ›å»ºå¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆæ¯ä¸ªé—®é¢˜ä¸€ä¸ªï¼‰
    expanded_samples = []
    for q, a in zip(questions, answers):
        text = facts + " " + rules + " " + q
        enc = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=512,
        )
        expanded_samples.append({
            "input_ids": enc["input_ids"],
            "attention_mask": enc["attention_mask"],
            "labels": 1 if a.strip() == "T" else 0,
        })

    return expanded_samples


def build_lora_config(model_name: str) -> LoraConfig:
    """
    æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ LoRA target_modules
    - å¯¹äº decoder-onlyï¼ˆQwen / LLaMA/TinyLlamaï¼‰ä½¿ç”¨ q/k/v/o_proj
    - å¯¹äº BERT è¿™ç±» encoder-onlyï¼Œä½¿ç”¨ query / value
    """
    lower_name = model_name.lower()
    if "llama" in lower_name or "qwen" in lower_name or "tinyllama" in lower_name:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
    else:
        target_modules = ["query", "value"]

    lora = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="SEQ_CLS",
    )
    return lora


def train(model_key: str = "bert"):
    model_name = MODEL_LIST[model_key]
    print(f"â–¶ Using base model: {model_name}")

    # ------------------ tokenizer ------------------
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Qwen / LLaMA / TinyLlama æœ‰å¯èƒ½æ²¡æœ‰ pad_tokenï¼Œè¿™é‡Œæ˜¾å¼è®¾ç½®ä¸º eos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        print(f"âš ï¸ pad_token was missing, set pad_token = eos_token = {tokenizer.pad_token}")

    # ------------------ dataset ------------------
    dataset = load_dataset("csv", data_files="data/train.csv")["train"]

    # FIXED: Expand each row into multiple samples (one per question)
    expanded_data = []
    for sample in dataset:
        expanded_samples = encode(sample, tokenizer)
        expanded_data.extend(expanded_samples)

    # Convert back to HuggingFace Dataset
    from datasets import Dataset
    dataset = Dataset.from_list(expanded_data)

    # ------------------ model ------------------
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
    )

    # ç¡®ä¿æ¨¡å‹ä¹ŸçŸ¥é“ pad_token_id
    model.config.pad_token_id = tokenizer.pad_token_id

    # ------------------ LoRA ------------------
    lora_config = build_lora_config(model_name)
    model = get_peft_model(model, lora_config)
    print("â–¶ LoRA enabled with config:", lora_config)

    # ------------------ training args ------------------
    output_dir = f"./trained_models/{model_key}"
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        num_train_epochs=3,
        learning_rate=2e-5,
        save_strategy="epoch",
        logging_steps=20,
        remove_unused_columns=False,  # æˆ‘ä»¬å·²ç»æ‰‹åŠ¨æ§åˆ¶è¾“å…¥åˆ—
    )

    # ------------------ trainer ------------------
    trainer = Trainer(
        model=model,
        args=args,
        tokenizer=tokenizer,
        train_dataset=dataset,
    )

    # ------------------ train ------------------
    print("ğŸš€ Start training...")
    trainer.train()

    # ------------------ save ------------------
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… Model saved to: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        type=str,
        default="bert",
        choices=["bert", "qwen", "llama"],
        help="Which base model to fine-tune",
    )
    args = parser.parse_args()

    train(args.model)
